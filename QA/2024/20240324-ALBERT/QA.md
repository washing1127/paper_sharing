# Transformer中的参数共享

# DQE那里提到的那句话是什么意思
“DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same.”

# ALBERT 和 BERT 在训练数据上有什么差异
